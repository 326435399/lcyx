<!DOCTYPE html>
<html lang="zh-cn">
    <head>
        <meta charset="UTF-8">
        <meta name="StrawBear" content="width=device-width,initial-scale=1.0">
        <title>深度学习入门-自制框架</title>
        <link rel="stylesheet" href="../../../1_CSS/basicStyle.css">
    </head>   
    <body>
        <aside>
            <ul>
                <li><a href="#1">1 作为"箱子"的变量</a>
                    <ul>
                        <li><a href="#1.1">1.1 什么是变量</a></li>
                        <li><a href="#1.2">1.2 实现Variable类</a></li>
                        <li><a href="#1.3">1.3 (补充)NumPy的多维数组</a></li>
                    </ul>
                </li>
                <li><a href="#2">2 创建变量的函数</a>
                    <ul>
                        <li><a href="#2.1">2.1 什么是函数</a></li>
                        <li><a href="#2.2">2.2 Function类的实现</a></li>
                        <li><a href="#2.3">2.3 使用功能Function类</a></li>
                    </ul>
                </li>
                <li><a href="#3">3 函数的连续调用</a>
                    <ul>
                        <li><a href="#3.1">3.1 Exp函数的实现</a></li>
                        <li><a href="#3.2">3.2 函数的连续调用</a></li>
                    </ul>
                </li>
                <li><a href="#4">4 数值微分</a>
                    <ul>
                        <li><a href="#4.1">4.1 什么是导数</a></li>
                        <li><a href="#4.2">4.2 数值微分的实现</a></li>
                        <li><a href="#4.3">4.3 复合函数的导数</a></li>
                        <li><a href="#4.4">4.4 数值微分存在的问题</a></li>
                    </ul>
                </li>
                <li><a href="#5">5 反向传播的理论知识</a>
                    <ul>
                        <li><a href="#5.1">5.1 链式法则</a></li>
                        <li><a href="#5.2">5.2 反向传播的推导</a></li>
                        <li><a href="#5.3">5.3 用计算图表示</a></li>
                    </ul>
                </li>
                <li><a href="#6">6 手动进行反向传播</a>
                    <ul>
                        <li><a href="#6.1">6.1 Variable类的功能扩展</a></li>
                        <li><a href="#6.2">6.2 Function类的功能扩展</a></li>
                        <li><a href="#6.3">6.3 Square类和Exp类的功能扩展</a></li>
                        <li><a href="#6.4">6.4 反向传播的实现</a></li>
                    </ul>
                </li>
                <li><a href="#7">7 反向传播的自动化</a>
                    <ul>
                        <li><a href="#7.1">7.1 为反向传播的自动化创造条件</a></li>
                        <li><a href="#7.2">7.2 尝试反向传播</a></li>
                        <li><a href="#7.3">7.3 增加backward方法</a></li>
                    </ul>
                </li>
                <li><a href="#8">8 从递归到循环</a>
                    <ul>
                        <li><a href="#8.1">8.1 现在的Variable类</a></li>
                        <li><a href="#8.2">8.2 使用功能循环实现</a></li>
                        <li><a href="#8.3">8.3 代码验证</a></li>
                    </ul>
                </li>
                <li><a href="#9">9 让函数更容易</a>
                    <ul>
                        <li><a href="#9.1">9.1 作为Python函数使用</a></li>
                        <li><a href="#9.2">9.2 简化backward方法</a></li>
                        <li><a href="#9.3">9.3 只支持ndarray</a></li>
                    </ul>
                </li>
                <li><a href="#10">10 测试</a>
                    <ul>
                        <li><a href="#10.1">10.1 Python的单元测试</a></li>
                        <li><a href="#10.2">10.2 square函数反向传播的测试</a></li>
                        <li><a href="#10.3">10.3 通过梯度检验来自动测试</a></li>
                        <li><a href="#10.4">10.4 测试小结</a></li>
                    </ul>
                </li>
                <li><a href="#11">11 可变长参数(正向传播篇)</a>
                    <ul>
                        <li><a href="#11.1">11.1 修改Function类</a></li>
                        <li><a href="#11.2">11.2 Add类的实现</a></li>
                    </ul>
                </li>
                <li><a href="#12">12 可变长参数(改进篇)</a>
                    <ul>
                        <li><a href="#12.1">12.1 第1项改进:使函数更容易使用</a></li>
                        <li><a href="#12.2">12.2 第2项改进:使函数更容易使用</a></li>
                        <li><a href="#12.3">12.3 add函数的实现</a></li>
                    </ul>
                </li>
                <li><a href="#13">13 可变长参数(反向传播篇)</a>
                    <ul>
                        <li><a href="#13.1">13.1 支持可变长参数的Add类的反向传播</a></li>
                        <li><a href="#13.2">13.2 修改Variable类</a></li>
                        <li><a href="#13.3">13.3 Square类的实现</a></li>
                    </ul>
                </li>
                <li><a href="#14">14 重复使用同一个变量</a>
                    <ul>
                        <li><a href="#14.1">14.1 问题的原因</a></li>
                        <li><a href="#14.2">14.2 解决方案</a></li>
                        <li><a href="#14.3">14.3 重置导数</a></li>
                    </ul>
                </li>
                <li><a href="#15">15 复杂的计算图(理论篇)</a>
                    <ul>
                        <li><a href="#15.1">15.1 反向传播的正确顺序</a></li>
                        <li><a href="#15.2">15.2 当前的DeZero</a></li>
                        <li><a href="#15.3">15.3 函数的优先级</a></li>
                    </ul>
                </li>
                <li><a href="#16">16 复杂的计算图(实现篇)</a>
                    <ul>
                        <li><a href="#16.1">16.1 增加"辈分"变量</a></li>
                        <li><a href="#16.2">16.2 按照"辈分"顺序取出元素</a></li>
                        <li><a href="#16.3">16.3 Variable类的backward</a></li>
                        <li><a href="#16.4">16.4 代码验证</a></li>
                    </ul>
                </li>
                <li><a href="#17">17 内存管理和循环引用</a>
                    <ul>
                        <li><a href="#17.1">17.1 内存管理</a></li>
                        <li><a href="#17.2">17.2 引用计数方式的内存管理</a></li>
                        <li><a href="#17.3">17.3 循环引用</a></li>
                        <li><a href="#17.4">17.4 weakref模块</a></li>
                        <li><a href="#17.5">17.5 代码验证</a></li>
                    </ul>
                </li>
                <li><a href="#18">18 减少内存使用量的模式</a>
                    <ul>
                        <li><a href="#18.1">18.1 不保留不必要的导数</a></li>
                        <li><a href="#18.2">18.2 回顾Function类</a></li>
                        <li><a href="#18.3">18.3 使用Config类进行切换</a></li>
                        <li><a href="#18.4">18.4 模式的切换</a></li>
                        <li><a href="#18.5">18.5 使用with语句切换</a></li>
                    </ul>
                </li>
                <li><a href="#19">19 让变量更容易</a>
                    <ul>
                        <li><a href="#19.1">19.1 命名变量</a></li>
                        <li><a href="#19.2">19.2 示例变量ndarray</a></li>
                        <li><a href="#19.3">19.3 len函数和print函数</a></li>
                    </ul>
                </li>
                <li><a href="#20">20 运算符重载(1)</a>
                    <ul>
                        <li><a href="#20.1">20.1 Mul类的实现</a></li>
                        <li><a href="#20.2">20.2 运算符重载</a></li>
                    </ul>
                </li>
                <li><a href="#21">21 运算符重载(2)</a>
                    <ul>
                        <li><a href="#21.1">21.1 与ndarray一起使用</a></li>
                        <li><a href="#21.2">21.2 与float和int一起使用</a></li>
                        <li><a href="#21.3">21.3 问题1:左项为float或int的情况</a></li>
                        <li><a href="#21.4">21.4 问题2:左项为ndarray实例的情况</a></li>
                    </ul>
                </li>
                <li><a href="#22">22 运算符重载(3)</a>
                    <ul>
                        <li><a href="#22.1">22.1 负数</a></li>
                        <li><a href="#22.2">22.2 减法</a></li>
                        <li><a href="#22.3">22.3 除法</a></li>
                        <li><a href="#22.4">22.4 幂运算</a></li>
                    </ul>
                </li>
                <li><a href="#23">23 打包</a>
                    <ul>
                        <li><a href="#23.1">23.1 文件结构</a></li>
                        <li><a href="#23.2">23.2 将代码移到核心类</a></li>
                        <li><a href="#23.3">23.3 运算符重载</a></li>
                        <li><a href="#23.4">23.4 实际的__init__.py文件</a></li>
                        <li><a href="#23.5">23.5 导入dezero</a></li>
                    </ul>
                </li>
                <li><a href="#24">24 复杂函数的求导</a>
                    <ul>
                        <li><a href="#24.1">24.1 Sphere函数</a></li>
                        <li><a href="#24.2">24.2 matyas函数</a></li>
                        <li><a href="#24.3">24.3 Goldstein-Price函数</a></li>
                    </ul>
                </li>
                <li><a href="#25">25 计算图的可视化(1)</a>
                    <ul>
                        <li><a href="#25.1">25.1 安装Graphviz</a></li>
                        <li><a href="#25.2">25.2 使用DOT语言描述图形</a></li>
                        <li><a href="#25.3">25.3 指定节点属性</a></li>
                        <li><a href="#25.4">25.4 连接节点</a></li>
                    </ul>
                </li>
                <li><a href="#26">26 计算图的可视化(2)</a>
                    <ul>
                        <li><a href="#26.1">26.1 可视化代码的使用示例</a></li>
                        <li><a href="#26.2">26.2 从计算图转换为DOT语言</a></li>
                        <li><a href="#26.3">26.3 从DOT语言转换为图像</a></li>
                        <li><a href="#26.4">26.4 代码验证</a></li>
                    </ul>
                </li>
                <li><a href="#27">27 泰勒展开的导数</a>
                    <ul>
                        <li><a href="#27.1">27.1 sin函数的实现</a></li>
                        <li><a href="#27.2">27.2 泰勒展开的理论知识</a></li>
                        <li><a href="#27.3">27.3 泰勒展开的实现</a></li>
                        <li><a href="#27.4">27.4 计算图的可视化</a></li>
                    </ul>
                </li>
                <li><a href="#28">28 函数优化</a>
                    <ul>
                        <li><a href="#28.1">28.1 Rosenbrock函数</a></li>
                        <li><a href="#28.2">28.2 求导</a></li>
                        <li><a href="#28.3">28.3 梯度下降法的实现</a></li>
                    </ul>
                </li>
                <li><a href="#29">29 使用牛顿法进行优化(手动计算)</a>
                    <ul>
                        <li><a href="#29.1">29.1 使用牛顿法进行优化的理论知识</a></li>
                        <li><a href="#29.2">29.2 使用牛顿法实现优化</a></li>
                    </ul>
                </li>
                <li><a href="#30">30 高阶导数(准备篇)</a>
                    <ul>
                        <li><a href="#30.1">30.1 确认工作1:Variable实例变量</a></li>
                        <li><a href="#30.2">30.2 确认工作2:Function类</a></li>
                        <li><a href="#30.3">30.3 确认工作3:Variable类的反向传播</a></li>
                    </ul>
                </li>
                <li><a href="#31">31 高阶导数(理论篇)</a>
                    <ul>
                        <li><a href="#31.1">31.1 在反向传播时进行的计算</a></li>
                        <li><a href="#31.2">31.2 创建反向传播的计算图的方法</a></li>
                    </ul>
                </li>
                <li><a href="#32">32 高阶导数(实现篇)</a>
                    <ul>
                        <li><a href="#32.1">32.1 新的DeZero</a></li>
                        <li><a href="#32.2">32.2 函数类的反向传播</a></li>
                        <li><a href="#32.3">32.3 实现更有效的反向传播(增加模式控制代码)</a></li>
                        <li><a href="#32.4">32.4 修改__init__.py</a></li>
                    </ul>
                </li>
                <li><a href="#33">33 使用牛顿法进行优化(自动计算)</a>
                    <ul>
                        <li><a href="#33.1">33.1 求二阶导数</a></li>
                        <li><a href="#33.2">33.2 使用牛顿法进行优化</a></li>
                    </ul>
                </li>
                <li><a href="#34">34 sin函数的高阶导数</a>
                    <ul>
                        <li><a href="#34.1">34.1 sin函数的实现</a></li>
                        <li><a href="#34.2">34.2 cos函数的实现</a></li>
                        <li><a href="#34.3">34.3 sin函数的高阶导数</a></li>
                    </ul>
                </li>
                <li><a href="#35">35 高阶导数的计算图</a>
                    <ul>
                        <li><a href="#35.1">35.1 tanh函数的导数</a></li>
                        <li><a href="#35.2">35.2 tanh函数的实现</a></li>
                        <li><a href="#35.3">35.3 高阶导数的计算图可视化</a></li>
                    </ul>
                </li>
                <li><a href="#36">36 DeZero的其他用途</a>
                    <ul>
                        <li><a href="#36.1">36.1 double backprop的用途</a></li>
                        <li><a href="#36.2">36.2 深度学习研究中的应用示例</a></li>
                    </ul>
                </li>
                <li><a href="#37">37 处理张量</a>
                    <ul>
                        <li><a href="#37.1">37.1 对个元素进行计算</a></li>
                        <li><a href="#37.2">37.2 使用张量时的反向传播</a></li>
                        <li><a href="#37.3">37.3 使用张量时的反向传播(补充内容)</a></li>
                    </ul>
                </li>
                <li><a href="#38">38 改变形状的函数</a>
                    <ul>
                        <li><a href="#38.1">38.1 reshape函数的实现</a></li>
                        <li><a href="#38.2">38.2 从Variable对象调用reshape</a></li>
                        <li><a href="#38.3">38.3 矩阵的转置</a></li>
                        <li><a href="#38.4">38.4 实际的transpose函数(补充内容)</a></li>
                    </ul>
                </li>
                <li><a href="#39">39 求和的函数</a>
                    <ul>
                        <li><a href="#39.1">39.1 sum函数的反向传播</a></li>
                        <li><a href="#39.2">39.2 sum函数的实现</a></li>
                        <li><a href="#39.3">39.3 axis和keepdims</a></li>
                    </ul>
                </li>
                <li><a href="#40">40 进行广播的函数</a>
                    <ul>
                        <li><a href="#40.1">40.1 broadcast_to函数和sum_to函数</a></li>
                        <li><a href="#40.2">40.2 DeZero的broadcast_to函数和sum_to函数</a></li>
                        <li><a href="#40.3">40.3 支持广播</a></li>
                    </ul>
                </li>
                <li><a href="#41">41 矩阵的乘积</a>
                    <ul>
                        <li><a href="#41.1">41.1 向量的内积和矩阵的乘积</a></li>
                        <li><a href="#41.2">41.2 检查矩阵的形状</a></li>
                        <li><a href="#41.3">41.3 矩阵乘积的反向传播</a></li>
                    </ul>
                </li>
                <li><a href="#42">42 线性回归</a>
                    <ul>
                        <li><a href="#42.1">42.1 玩具数据集</a></li>
                        <li><a href="#42.2">42.2 线性回归的理论知识</a></li>
                        <li><a href="#42.3">42.3 线性回归的实现</a></li>
                        <li><a href="#42.4">42.4 DeZero的mean_squared_error函数(补充内容)</a></li>
                    </ul>
                </li>
                <li><a href="#43">43 神经网络</a>
                    <ul>
                        <li><a href="#43.1">43.1 DeZero中的linear函数</a></li>
                        <li><a href="#43.2">43.2 非线性数据集</a></li>
                        <li><a href="#43.3">43.3 激活函数和神经网络</a></li>
                        <li><a href="#43.4">43.4 神经网络的实现</a></li>
                    </ul>
                </li>
                <li><a href="#44">44 汇总参数的层</a>
                    <ul>
                        <li><a href="#44.1">44.1 Parameter类的实现</a></li>
                        <li><a href="#44.2">44.2 Layer类的实现</a></li>
                        <li><a href="#44.3">44.3 Linear类的实现</a></li>
                        <li><a href="#44.4">44.4 使用Layer实现神经网络</a></li>
                    </ul>
                </li>
                <li><a href="#45">45 汇总层的层</a>
                    <ul>
                        <li><a href="#45.1">45.1 扩展Layer类</a></li>
                        <li><a href="#45.2">45.2 Model类</a></li>
                        <li><a href="#45.3">45.3 使用Model来解决问题</a></li>
                        <li><a href="#45.4">45.4 MLP类</a></li>
                    </ul>
                </li>
                <li><a href="#46">46 通过Optimizer更新参数</a>
                    <ul>
                        <li><a href="#46.1">46.1 Optimizer类</a></li>
                        <li><a href="#46.2">46.2 SGD类的实现</a></li>
                        <li><a href="#46.3">46.3 使用SGD类来解决问题</a></li>
                        <li><a href="#46.4">46.4 SGD以外的优化方法</a></li>
                    </ul>
                </li>
                <li><a href="#47">47 softmax函数和交叉熵误差</a>
                    <ul>
                        <li><a href="#47.1">47.1 用于切片操作的函数</a></li>
                        <li><a href="#47.2">47.2 softmax函数</a></li>
                        <li><a href="#47.3">47.3 交叉熵误差</a></li>
                    </ul>
                </li>
                <li><a href="#48">48 多分类</a>
                    <ul>
                        <li><a href="#48.1">48.1 螺旋数据集</a></li>
                        <li><a href="#48.2">48.2 用于训练的代码</a></li>
                    </ul>
                </li>
                <li><a href="#49">49 Dataset类和预处理</a>
                    <ul>
                        <li><a href="#49.1">49.1 Dataset类的实现</a></li>
                        <li><a href="#49.2">49.2 大型数据集的情况</a></li>
                        <li><a href="#49.3">49.3 数据的连接</a></li>
                        <li><a href="#49.4">49.4 用于训练的代码</a></li>
                        <li><a href="#49.5">49.5 数据集的预处理</a></li>
                    </ul>
                </li>
                <li><a href="#50">50 用于取出小批量数据的DataLoader</a>
                    <ul>
                        <li><a href="#50.1">50.1 什么是迭代器</a></li>
                        <li><a href="#50.2">50.2 使用DataLoader</a></li>
                        <li><a href="#50.3">50.3 accuracy函数的实现</a></li>
                        <li><a href="#50.4">50.4 螺旋数据集的训练代码</a></li>
                    </ul>
                </li>
                <li><a href="#51">51 MINST的训练</a>
                    <ul>
                        <li><a href="#51.1">51.1 MNIST数据集</a></li>
                        <li><a href="#51.2">51.2 训练MNIST</a></li>
                        <li><a href="#51.3">51.3 改进模型</a></li>
                    </ul>
                </li>
                <li><a href="#52">52 支持GPU</a>
                    <ul>
                        <li><a href="#52.1">52.1 CuPy的安装和使用方法</a></li>
                        <li><a href="#52.2">52.2 cuda模块</a></li>
                        <li><a href="#52.3">52.3 向Variable/Layer/DataLoader类添加代码</a></li>
                        <li><a href="#52.4">52.4 函数的相应修改</a></li>
                        <li><a href="#52.5">52.5 在GPU上训练MNIST</a></li>
                    </ul>
                </li>
                <li><a href="#53">53 模型的保存和加载</a>
                    <ul>
                        <li><a href="#53.1">53.1 NumPy的save函数和load函数</a></li>
                        <li><a href="#53.2">53.2 Layer类参数的扁平化</a></li>
                        <li><a href="#53.3">53.3 Layer类的save函数和load函数</a></li>
                    </ul>
                </li>
                <li><a href="#54">54 Dropout和测试模式</a>
                    <ul>
                        <li><a href="#54.1">54.1 什么是Dropout</a></li>
                        <li><a href="#54.2">54.2 Inverted Dropout</a></li>
                        <li><a href="#54.3">54.3 增加测试模式</a></li>
                        <li><a href="#54.4">54.4 Dropout的实现</a></li>
                    </ul>
                </li>
                <li><a href="#55">55 CNN的机制(1)</a>
                    <ul>
                        <li><a href="#55.1">55.1 CNN的网络结构</a></li>
                        <li><a href="#55.2">55.2 卷积运算</a></li>
                        <li><a href="#55.3">55.3 填充</a></li>
                        <li><a href="#55.4">55.4 步幅</a></li>
                        <li><a href="#55.5">55.5 输出大小的计算方法</a></li>
                    </ul>
                </li>
                <li><a href="#56">56 CNN的机制(2)</a>
                    <ul>
                        <li><a href="#56.1">56.1 三阶张量</a></li>
                        <li><a href="#56.2">56.2 结合方块进行思考</a></li>
                        <li><a href="#56.3">56.3 小批量处理</a></li>
                        <li><a href="#56.4">56.4 池化层</a></li>
                    </ul>
                </li>
                <li><a href="#57">57 conv2d函数和pooling函数</a>
                    <ul>
                        <li><a href="#57.1">57.1 使用im2col展开</a></li>
                        <li><a href="#57.2">57.2 conv2d函数的实现</a></li>
                        <li><a href="#57.3">57.3 Conv2d层的实现</a></li>
                        <li><a href="#57.4">57.4 pooling函数的实现</a></li>
                    </ul>
                </li>
                <li><a href="#58">58 具有代表性的CNN(VGG16)</a>
                    <ul>
                        <li><a href="#58.1">58.1 VGG16的实现</a></li>
                        <li><a href="#58.2">58.2 已训练的权重数据</a></li>
                        <li><a href="#58.3">58.3 使用已训练的VGG16</a></li>
                    </ul>
                </li>
                <li><a href="#59">59 使用RNN处理时间序列数据</a>
                    <ul>
                        <li><a href="#59.1">59.1 RNN层的实现</a></li>
                        <li><a href="#59.2">59.2 RNN模型的实现</a></li>
                        <li><a href="#59.3">59.3 切断连接的方式</a></li>
                        <li><a href="#59.4">59.4 正弦波的预测</a></li>
                    </ul>
                </li>
                <li><a href="#60">60 LSTM与数据加载器</a>
                    <ul>
                        <li><a href="#60.1">60.1 用于时间序列数据的数据加载器</a></li>
                        <li><a href="#60.2">60.2 LSTM层的实现</a></li>

                    </ul>
                </li>
            </ul>
        </aside>
        <nav>
            <a href="#20240505"><img style="width:18pt" src="Resources/other/跳转.jpg" alt="跳转"></a>20240505:苹果农场优化<br>            
            在前面的学习当中小熊用了Paramater和Layer,Model方法来对整个苹果筛选工艺进行了整理.<br>
            我们是通过手动调整参数的方式来优化这些工艺,为了方便以后可以自由的更换工艺,采购了一个机器人Optimizers后面简称Op来统一管理工艺参数调优.<br>
            机器人Op可以在网络上下载别人设置好的优化方案,也可以自己手动编写优化方案,只要是相同类型的机器都可以快速切换不同的优化方案,实现了快速调整机器参数优化方案的功能.<br>
            现在我们采用的都是传统的SGD随机梯度下降优化方案,有人提出了MomentumSGD动量随机梯度下降方案,在某些情况下拥有比SGD更好的优化效果.<br>
            <a href="#20240507"><img style="width:18pt" src="Resources/other/跳转.jpg" alt="跳转"></a>20240507-苹果分类问题:<br>
            为了对不同区域苹果树品质进行区分,我们调用了Op机器人的新的损失函数,<strong>交叉熵误差</strong>,通过对苹果品质的统计得到概率,转换概率
            使用的函数是<strong>softmax</strong>,使用<strong>get_item</strong>函数来对不同的树木组数据来切片对比分析,最后得到了不同区域不同切片组的
            树木的苹果品质结果,针对性的对品质较差的树木进行重点改善提高品质<br>  
            <a href="#20240510"><img style="width:18pt" src="Resources/other/跳转.jpg" alt="跳转"></a>20240510-螺旋小批量<br>
            因为苹果数量比较多,一次性质检太过工作量非常巨大,小熊采用了<strong>螺旋</strong>选择方式,对范围内的苹果随机采样,然后对部分采样<strong>小批量</strong>质量检验,得到一个近似准确的统计结果<br>
            <a href="#20240513"><img style="width:18pt" src="Resources/other/跳转.jpg" alt="跳转"></a>20240513-苹果采样器<br>
            苹果的数量越来越多,为了更好的质检,设计了一个叫做<strong>Dataset</strong>的工具来处理所有的苹果,实现小批量的加载,
            由于控制界面复杂,又设计了<strong>DataLoader</strong>来更加容易使用<br>,现在只需要加载符合Dataset格式的数据,然后调用DataLoader来读取,
            最后看返回的<strong>accuracy</strong>就知道准确度了<br>
            <a href="#20240514"><img style="width:18pt" src="Resources/other/跳转.jpg" alt="跳转"></a>20240514-MNIST<br>
            图像识别数据集<strong>MNIST</strong>,采用了新的<strong>ReLU</strong>激活函数来进行识别图像<br>
            <a href="#20240515"><img style="width:18pt" src="Resources/other/跳转.jpg" alt="跳转"></a>20240515-Cupy<br>
            拥有了最新RTX4090Ti的小熊想要使用<strong>GPU</strong>来训练,首先需要下载Python的<strong>Cupy库</strong>,然后准备一系列的自动判断是否支持GPU的函数,
            所有的处理代码都增加自动切换功能就实现了GPU训练<br>
            <a href="#20240516"><img style="width:18pt" src="Resources/other/跳转.jpg" alt="跳转"></a>20240516-模型保存和加载<br>
            我们可以通过Layer的load_weights和save_weights调用Numpy的load和save函数来实现模型的加载和保存给以后使用<br>
            <a href="#20240520"><img style="width:18pt" src="Resources/other/跳转.jpg" alt="跳转"></a>20240520-过拟合和CNN<br>
            面对神经网络的过拟合问题,我们使用<strong>Dropout</strong>来近似集成学习<strong>减少过拟合</strong>,
            <strong>CNN卷积神经网络</strong>新增了<strong>卷积层</strong>和<strong>池化层</strong>,卷积层的一些重要概念是<strong>卷积核,偏置</strong>.<br>
            <a href="#20240521"><img style="width:18pt" src="Resources/other/跳转.jpg" alt="跳转"></a>20240521-池化层和im2col<br>
            我们使用<strong>池化层</strong>来对一批苹果当中选出最大的或者平均大小,通过<strong>im2col</strong>函数把苹果根据池化层的大小来分开打包,方便管理<br>
            <a href="#20240522"><img style="width:18pt" src="Resources/other/跳转.jpg" alt="跳转"></a>20240522-VGG16和RNN<br>
            <strong>VGG16和RNN</strong>分别对应两种算法,VGG16通过多个卷积层来实现图像识别,RNN通过循环利用上次计算的输出来建立数据之间的联系<br>
        </nav>
        <header><h1 class="header">深度学习入门-自制框架</h1></header>
        <article id="1"><h2 class="chapter">1 作为"箱子"的变量</h2>
            <section id="1.1">1.1 什么是变量</section>
            <p></p>
            <section id="1.2">1.2 实现Variable类</section>
            <p></p>
            <section id="1.3">1.3 (补充)NumPy的多维数组</section>
            <p></p>
        </article>
        <article id="2"><h2 class="chapter">2 创建变量的函数</h2>
            <section id="2.1">2.1 什么是函数</section>
            <p></p>
            <section id="2.2">2.2 Function类的实现</section>
            <p></p>
            <section id="2.3">2.3 使用功能Function类</section>
            <p></p>
        </article>
        <article id="3"><h2 class="chapter">3 函数的连续调用</h2>
            <section id="3.1">3.1 Exp函数的实现</section>
            <p></p>
            <section id="3.2">3.2 函数的连续调用</section>
            <p></p>
        </article>
        <article id="4"><h2 class="chapter">4 数值微分</h2>
            <section id="4.1">4.1 什么是导数</section>
            <p></p>
            <section id="4.2">4.2 数值微分的实现</section>
            <p></p>
            <section id="4.3">4.3 复合函数的导数</section>
            <p></p>
            <section id="4.4">4.4 数值微分存在的问题</section>
            <p></p>
        </article>
        <article id="5"><h2 class="chapter">5 反向传播的理论知识</h2>
            <section id="5.1">5.1 链式法则</section>
            <p></p>
            <section id="5.2">5.2 反向传播的推导</section>
            <p></p>
            <section id="5.3">5.3 用计算图表示</section>
            <p></p>
        </article>
        <article id="6"><h2 class="chapter">6 手动进行反向传播</h2>
            <section id="6.1">6.1 Variable类的功能扩展</section>
            <p></p>
            <section id="6.2">6.2 Function类的功能扩展</section>
            <p></p>
            <section id="6.3">6.3 Square类和Exp类的功能扩展</section>
            <p></p>
            <section id="6.4">6.4 反向传播的实现</section>
            <p></p>
        </article>
        <article id="7"><h2 class="chapter">7 反向传播的自动化</h2>
            <section id="7.1">7.1 为反向传播的自动化创造条件</section>
            <p></p>
            <section id="7.2">7.2 尝试反向传播</section>
            <p></p>
            <section id="7.3">7.3 增加backward方法</section>
            <p></p>
        </article>
        <article id="8"><h2 class="chapter">8 从递归到循环</h2>
            <section id="8.1">8.1 现在的Variable类</section>
            <p></p>
            <section id="8.2">8.2 使用功能循环实现</section>
            <p></p>
            <section id="8.3">8.3 代码验证</section>
            <p></p>
        </article>
        <article id="9"><h2 class="chapter">9 让函数更容易</h2>
            <section id="9.1">9.1 作为Python函数使用</section>
            <p></p>
            <section id="9.2">9.2 简化backward方法</section>
            <p></p>
            <section id="9.3">9.3 只支持ndarray</section>
            <p></p>
        </article>
        <article id="10"><h2 class="chapter">10 测试</h2>
            <section id="10.1">10.1 Python的单元测试</section>
            <p></p>
            <section id="10.2">10.2 square函数反向传播的测试</section>
            <p></p>
            <section id="10.3">10.3 通过梯度检验来自动测试</section>
            <p></p>
            <section id="10.4">10.4 测试小结</section>
            <p></p>
        </article>
        <article id="11"><h2 class="chapter">11 可变长参数(正向传播篇)</h2>
            <section id="11.1">11.1 修改Function类</section>
            <p></p>
            <section id="11.1">11.2 Add类的实现</section>
            <p></p>
        </article>
        <article id="12"><h2 class="chapter">12 可变长参数(改进篇)</h2>
            <section id="12.1">12.1 第1项改进:使函数更容易使用</section>
            <p></p>
            <section id="12.2">12.2 第2项改进:使函数更容易使用</section>
            <p></p>
            <section id="12.3">12.3 add函数的实现</section>
            <p></p>
        </article>
        <article id="13"><h2 class="chapter">13 可变长参数(反向传播篇)</h2>
            <section id="13.1">13.1 支持可变长参数的Add类的反向传播</section>
            <p></p>
            <section id="13.2">13.2 修改Variable类</section>
            <p></p>
            <section id="13.3">13.3 Square类的实现</section>
            <p></p>
        </article>
        <article id="14"><h2 class="chapter">14 重复使用同一个变量</h2>
            <section id="14.1">14.1 问题的原因</section>
            <p></p>
            <section id="14.2">14.2 解决方案</section>
            <p></p>
            <section id="14.3">14.3 重置导数</section>
            <p></p>
        </article>
        <article id="15"><h2 class="chapter">15 复杂的计算图(理论篇)</h2>
            <section id="15.1">15.1 反向传播的正确顺序</section>
            <p></p>
            <section id="15.2">15.2 当前的DeZero</section>
            <p></p>
            <section id="15.3">15.3 函数的优先级</section>
            <p></p>
        </article>
        <article id="16"><h2 class="chapter">16 复杂的计算图(实现篇)</h2>
            <section id="16.1">16.1 增加"辈分"变量</section>
            <p></p>
            <section id="16.2">16.2 按照"辈分"顺序取出元素</section>
            <p></p>
            <section id="16.3">16.3 Variable类的backward</section>
            <p></p>
            <section id="16.4">16.4 代码验证</section>
            <p></p>
        </article>
        <article id="17"><h2 class="chapter">17 内存管理和循环引用</h2>
            <section id="17.1">17.1 内存管理</section>
            <p></p>
            <section id="17.2">17.2 引用计数方式的内存管理</section>
            <p></p>
            <section id="17.3">17.3 循环引用</section>
            <p></p>
            <section id="17.4">17.4 weakref模块</section>
            <p></p>
            <section id="17.5">17.5 代码验证</section>
            <p></p>
        </article>
        <article id="18"><h2 class="chapter">18 减少内存使用量的模式</h2>
            <section id="18.1">18.1 不保留不必要的导数</section>
            <p></p>
            <section id="18.2">18.2 回顾Function类</section>
            <p></p>
            <section id="18.3">18.3 使用Config类进行切换</section>
            <p></p>
            <section id="18.4">18.4 模式的切换</section>
            <p></p>
            <section id="18.5">18.5 使用with语句切换</section>
            <p></p>
        </article>
        <article id="19"><h2 class="chapter">19 让变量更容易</h2>
            <section id="19.1">19.1 命名变量</section>
            <p></p>
            <section id="19.2">19.2 示例变量ndarray</section>
            <p></p>
            <section id="19.3">19.3 len函数和print函数</section>
            <p></p>
        </article>
        <article id="20"><h2 class="chapter">20 运算符重载(1)</h2>
            <section id="20.1">20.1 Mul类的实现</section>
            <p></p>
            <section id="20.2">20.2 运算符重载</section>
            <p></p>
        </article>
        <article id="21"><h2 class="chapter">21 运算符重载(2)</h2>
            <section id="21.1">21.1 与ndarray一起使用</section>
            <p></p>
            <section id="21.2">21.2 与float和int一起使用</section>
            <p></p>
            <section id="21.3">21.3 问题1:左项为float或int的情况</section>
            <p></p>
            <section id="21.4">21.4 问题2:左项为ndarray实例的情况</section>
            <p></p>
        </article>
        <article id="22"><h2 class="chapter">22 运算符重载(3)</h2>
            <section id="22.1">22.1 负数</section>
            <p></p>
            <section id="22.2">22.2 减法</section>
            <p></p>
            <section id="22.3">22.3 除法</section>
            <p></p>
            <section id="22.4">22.4 幂运算</section>
            <p></p>
        </article>
        <article id="23"><h2 class="chapter">23 打包</h2>
            <section id="23.1">23.1 文件结构</section>
            <p></p>
            <section id="23.2">23.2 将代码移到核心类</section>
            <p></p>
            <section id="23.3">23.3 运算符重载</section>
            <p></p>
            <section id="23.4">23.4 实际的__init__.py文件</section>
            <p></p>
            <section id="23.5">23.5 导入dezero</section>
            <p></p>
        </article>
        <article id="24"><h2 class="chapter">24 复杂函数的求导</h2>
            <section id="24.1">24.1 Sphere函数</section>
            <p></p>
            <section id="24.2">24.2 matyas函数</section>
            <p></p>
            <section id="24.3">24.3 Goldstein-Price函数</section>
            <p></p>
        </article>
        <article id="25"><h2 class="chapter">25 计算图的可视化(1)</h2>
            <section id="25.1">25.1 安装Graphviz</section>
            <p></p>
            <section id="25.2">25.2 使用DOT语言描述图形</section>
            <p></p>
            <section id="25.3">25.3 指定节点属性</section>
            <p></p>
            <section id="25.4">25.4 连接节点</section>
            <p></p>
        </article>
        <article id="26"><h2 class="chapter">26 计算图的可视化(2)</h2>
            <section id="26.1">26.1 可视化代码的使用示例</section>
            <p></p>
            <section id="26.2">26.2 从计算图转换为DOT语言</section>
            <p></p>
            <section id="26.3">26.3 从DOT语言转换为图像</section>
            <p></p>
            <section id="26.4">26.4 代码验证</section>
            <p></p>
        </article>
        <article id="27"><h2 class="chapter">27 泰勒展开的导数</h2>
            <section id="27.1">27.1 sin函数的实现</section>
            <p></p>
            <section id="27.2">27.2 泰勒展开的理论知识</section>
            <p></p>
            <section id="27.3">27.3 泰勒展开的实现</section>
            <p></p>
            <section id="27.4">27.4 计算图的可视化</section>
            <p></p>
        </article>
        <article id="28"><h2 class="chapter">28 函数优化</h2>
            <section id="28.1">28.1 Rosenbrock函数</section>
            <p></p>
            <section id="28.2">28.2 求导</section>
            <p></p>
            <section id="28.3">28.3 梯度下降法的实现</section>
            <p></p>
        </article>
        <article id="29"><h2 class="chapter">29 使用牛顿法进行优化(手动计算)</h2>
            <section id="29.1">29.1 使用牛顿法进行优化的理论知识</section>
            <p></p>
            <section id="29.2">29.2 使用牛顿法实现优化</section>
            <p></p>
        </article>
        <article id="30"><h2 class="chapter">30 高阶导数(准备篇)</h2>
            <section id="30.1">30.1 确认工作1:Variable实例变量</section>
            <p></p>
            <section id="30.2">30.2 确认工作2:Function类</section>
            <p></p>
            <section id="30.3">30.3 确认工作3:Variable类的反向传播</section>
            <p></p>
        </article>
        <article id="31"><h2 class="chapter">31 高阶导数(理论篇)</h2>
            <section id="31.1">31.1 在反向传播时进行的计算</section>
            <p></p>
            <section id="31.2">31.2 创建反向传播的计算图的方法</section>
            <p></p>
        </article>
        <article id="32"><h2 class="chapter">32 高阶导数(实现篇)</h2>
            <section id="32.1">32.1 新的DeZero</section>
            <p></p>
            <section id="32.2">32.2 函数类的反向传播</section>
            <p></p>
            <section id="32.3">32.3 实现更有效的反向传播(增加模式控制代码)</section>
            <p></p>
            <section id="32.4">32.4 修改__init__.py</section>
            <p></p>
        </article>
        <article id="33"><h2 class="chapter">33 使用牛顿法进行优化(自动计算)</h2>
            <section id="33.1">33.1 求二阶导数</section>
            <p></p>
            <section id="33.2">33.2 使用牛顿法进行优化</section>
            <p></p>
        </article>
        <article id="34"><h2 class="chapter">34 sin函数的高阶导数</h2>
            <section id="34.1">34.1 sin函数的实现</section>
            <p></p>
            <section id="34.2">34.2 cos函数的实现</section>
            <p></p>
            <section id="34.3">34.3 sin函数的高阶导数</section>
            <p></p>
        </article>
        <article id="35"><h2 class="chapter">35 高阶导数的计算图</h2>
            <section id="35.1">35.1 tanh函数的导数</section>
            <p></p>
            <section id="35.2">35.2 tanh函数的实现</section>
            <p></p>
            <section id="35.3">35.3 高阶导数的计算图可视化</section>
            <p></p>
        </article>
        <article id="36"><h2 class="chapter">36 DeZero的其他用途</h2>
            <section id="36.1">36.1 double backprop的用途</section>
            <p></p>
            <section id="36.2">36.2 深度学习研究中的应用示例</section>
            <p></p>
        </article>
        <article id="37"><h2 class="chapter">37 处理张量</h2>
            <section id="37.1">37.1 对个元素进行计算</section>
            <p></p>
            <section id="37.2">37.2 使用张量时的反向传播</section>
            <p></p>
            <section id="37.3">37.3 使用张量时的反向传播(补充内容)</section>
            <p></p>
        </article>
        <article id="38"><h2 class="chapter">38 改变形状的函数</h2>
            <section id="38.1">38.1 reshape函数的实现</section>
            <p></p>
            <section id="38.2">38.2 从Variable对象调用reshape</section>
            <p></p>
            <section id="38.3">38.3 矩阵的转置</section>
            <p></p>
            <section id="38.4">38.4 实际的transpose函数(补充内容)</section>
            <p></p>
        </article>
        <article id="39"><h2 class="chapter">39 求和的函数</h2>
            <section id="39.1">39.1 sum函数的反向传播</section>
            <p></p>
            <section id="39.2">39.2 sum函数的实现</section>
            <p></p>
            <section id="39.3">39.3 axis和keepdims</section>
            <p></p>
        </article>
        <article id="40"><h2 class="chapter">40 进行广播的函数</h2>
            <section id="40.1">40.1 broadcast_to函数和sum_to函数</section>
            <p></p>
            <section id="40.2">40.2 DeZero的broadcast_to函数和sum_to函数</section>
            <p></p>
            <section id="40.3">40.3 支持广播</section>
            <p></p>
        </article>
        <article id="41"><h2 class="chapter">41 矩阵的乘积</h2>
            <section id="41.1">41.1 向量的内积和矩阵的乘积</section>
            <p></p>
            <section id="41.2">41.2 检查矩阵的形状</section>
            <p></p>
            <section id="41.3">41.3 矩阵乘积的反向传播</section>
            <p></p>
        </article>
        <article id="42"><h2 class="chapter">42 线性回归</h2>
            <section id="42.1">42.1 玩具数据集</section>
            <p></p>
            <section id="42.2">42.2 线性回归的理论知识</section>
            <p></p>
            <section id="42.3">42.3 线性回归的实现</section>
            <p></p>
            <section id="42.4">42.4 DeZero的mean_squared_error函数(补充内容)</section>
            <p></p>
        </article>
        <article id="43"><h2 class="chapter">43 神经网络</h2>
            <section id="43.1">43.1 DeZero中的linear函数</section>
            <p></p>
            <section id="43.2">43.2 非线性数据集</section>
            <p></p>
            <section id="43.3">43.3 激活函数和神经网络</section>
            <p></p>
            <section id="43.4">43.4 神经网络的实现</section>
            <p></p>
        </article>
        <article id="44"><h2 class="chapter">44 汇总参数的层</h2>
            <section id="44.1">44.1 Parameter类的实现</section>
            <p></p>
            <section id="44.2">44.2 Layer类的实现</section>
            <p></p>
            <section id="44.3">44.3 Linear类的实现</section>
            <p></p>
            <section id="44.4">44.4 使用Layer实现神经网络</section>
            <p></p>
        </article>
        <article id="45"><h2 class="chapter">45 汇总层的层</h2>
            <section id="45.1">45.1 扩展Layer类</section>
            <p></p>
            <section id="45.2">45.2 Model类</section>
            <p></p>
            <section id="45.3">45.3 使用Model来解决问题</section>
            <p></p>
            <section id="45.4">45.4 MLP类</section>
            <p></p>
        </article>
        <article id="46"><h2 class="chapter">46 通过Optimizer更新参数</h2>
            <p class="timeStamp" id="20240505">20240505开始</p>
            <p>
                <strong>前言</strong><br>
                我们前面都是使用的梯度下降方法(SGD)来更新参数.在深度学习领域还有各种优化方法,在这一节我们将参数更新工作的代码模块化,方便后期轻松更换优化方法.<br>
            </p>
            <section id="46.1" class="section">46.1 Optimizer类</section>
            <p>
                把参数更新的基础类实现为Optimizer类.具体的优化类需要继承并实现它的抽象方法update_one.
                <figure>
                    <img src="Resources/img/46-1-1-Optimizers类.png" alt="Optimizers类">
                    <p>
                        初始化阶段:实例化了两个变量target和hooks,其中target是需要汇总参数的目标对象,hooks是需要预处理的函数.<br>
                        setup方法:设置target变量,返回设置好的Optimizer对象.<br>
                        update方法:将None之外的target对象的参数汇总,对hooks当中的函数执行预处理,对汇总的参数执行抽象方法update_one更新参数.<br>
                        add_hook方法:可选的添加哪些函数需要进行预处理,这个机制可以用于权重衰减,梯度裁剪等.<br>
                    </p>
                </figure>
            </p>
            <section id="46.2"  class="section">46.2 SGD类的实现</section>
            <p>
                SGD是Stochastic Gradient Descent的缩写--随机梯度下降法.<br>
                随机指的是从对象数据中随机选择数据实施梯度下降法.<br>
                <figure>
                    <img src="Resources/img/46-2-1-SGD类.png" alt="SGD类">
                    <p>
                        新增了自己的变量lr(learning rate)学习率,实现了抽象方法update_one,实现梯度下降操作.<br>
                        可以通过from dezero.optimizers import SGD从外部导入这个随机梯度下降方法了.<br>
                    </p>
                </figure>
            </p>
            <section id="46.3"  class="section">46.3 使用SGD类来解决问题</section>
            <p>
                现在使用我们准备好的SGD类来解决问题<br>
                <figure>
                    <img src="Resources/img/46-3-1-使用MLP和SGD.png" alt="使用MLP和SGD">
                    <p>
                        使用了MLP和SGD分别设置了模型和参数优化部分.<br>
                    </p>
                </figure>
            </p>
            <section id="46.4"  class="section">46.4 SGD以外的优化方法</section>
            <p>
                基于梯度的优化方法有很多,比较代表的有Momentum,AdaGrad,AdaDelata,Adam等.<br>
                <figure>
                    <img src="Resources/img/46-4-1-Momentum方法.png" alt="Momentum方法">
                    <p>
                        Momentum方法的理论,按照力学的方式在逐渐降低学习速率.<br>
                        式子46.1表示:物体在梯度方向上受到一个力,这个力使物体加速.<br>
                        式子46.2表示:物体以该速度进行移动<br>
                    </p>
                </figure>
                <figure>
                    <img src="Resources/img/46-4-2-Momentum实现.png" alt="Momentum实现">
                    <p>
                        实现方式也是相同的,通过继承Optimizer然后实现抽象方法update_one.<br>
                        后面就可以通过from dezero.optimizers import MomentumSGD来调用这个梯度方法了<br>
                        更换优化方法也是非常简单的,直接将SGD换成MomentumSGD就可以了.<br>
                    </p>
                </figure>
            </p>
        </article>
        <article id="47"><h2 class="chapter">47 softmax函数和交叉熵误差</h2>
            <p class="timeStamp" id="20240507">20240507开始</p>
            <p>
                我们现在需要处理多分类问题-将数据分类为多个值的问题.<br>
            </p>
            <section id="47.1" class="section">47.1 用于切片操作的函数
                <p>
                    <figure>
                        <img src="Resources/img/47-1-01-切片函数get_item.png" alt="切片函数get_item">
                        <p>
                            切片函数的实现,需要理解的是反向传播切片只是单纯的传递导数值<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/47-1-02-调用.png" alt="调用">
                        <p>
                            也可以使用get_item函数多次提取同一组元素,可以通过定义Variable.__getitem__=F.get_item让Variable也可以调用,原本的切片操作x[1]会调用我们准的这个函数了<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/47-1-03-计算图示例.png" alt="计算图示例">
                        <p>
                            注意反向传播先填0,然后再把指定切片的导数传递(1)<br>
                        </p>
                    </figure>
                </p>
            </section>
            <section id="47.2" class="section">47.2 softmax函数
                <p>
                    <figure>
                        <img src="Resources/img/47-2-01-softmax函数.png" alt="softmax函数">
                        <p>
                            softmax函数将值转换成概率<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/47-2-02-代码实现.png" alt="代码实现">
                        <p>
                            代码实现,后面可以调用这个函数来将输出值转换成概率了<br>
                        </p>
                    </figure>
                </p>
            </section>
            <section id="47.3" class="section">47.3 交叉熵误差
                <p>
                    <figure>
                        <img src="Resources/img/47-3-01-交叉熵误差.png" alt="交叉熵误差">
                        <p>
                            在线性回归中我们使用均方误差作为损失函数,多分类的时候使用功能的损失函数就是交叉熵误差.<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/47-3-02-代码实现.png" alt="代码实现">
                        <p>
                            代码实现,其中clip函数也是额外实现的,作用是控制p不要超出指定的范围<br>
                        </p>
                    </figure>
                </p>
            </section>
        </article>
        <article id="48"><h2 class="chapter">48 多分类</h2>
            <p class="timeStamp" id="20240510">20240510开始</p>
            <section id="48.1"><p class="section">48.1 螺旋数据集</p>
                <div>
                    <p class="paragraph"></p>
                    <p>
                        <figure>
                            <img src="Resources/img/48-01-螺旋数据集.png" alt="螺旋数据集">
                            <p>
                                train=True返回训练数据,等于False返回测试数据<br>
                            </p>
                        </figure>
                        <figure>
                            <img src="Resources/img/48-02-螺旋数据集.png" alt="螺旋数据集">
                            <p>
                                数据集有300个数据,每个数据是0 1 2 之间的一个t值<br>
                            </p>
                        </figure>
                    </p>
                </div>
            </section>
            <section id="48.2"><p class="section">48.2 用于训练的代码</p>
                <div>
                    <p class="paragraph"></p>
                    <p>
                        <figure>
                            <img src="Resources/img/48-03-代码.png" alt="代码">
                            <p>
                                中间一个比较特殊的做法是小批量,把大量的数据分批次加载<br>
                            </p>
                        </figure>
                        <figure>
                            <img src="Resources/img/48-04-损失函数图像.png" alt="损失函数图像">
                            <p>
                                损失函数的图像,明显能看到逐渐趋于平缓了<br>
                            </p>
                        </figure>
                        <figure>
                            <img src="Resources/img/48-05-计算结果.png" alt="计算结果">
                            <p>
                                最终训练的结果,能够识别出三种数据的边界了<br>
                            </p>
                        </figure>
                    </p>
                </div>
            </section>
        </article>
        <p class="timeStamp" id="20240513">20240513开始</p>
        <article id="49"><h2 class="chapter">49 Dataset类和预处理</h2>
            <section id="49.1">49.1 Dataset类的实现<p class="section"></p>
                <div>
                    <figure>
                        <img src="Resources/img/49-1-Dataset基类.png" alt="Dataset基类">
                        <p>
                            问题引出:上一节我们导入了一个300条数据的小数据集,所以我们可以当一个ndarray实例全部加载到内存,那么如果是100万个元素呢,那显然不太行,为了解决这种问题,创建一个专门用来做数据导入处理的Dataset类<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/49-2-玩具螺旋数据集.png" alt="玩具螺旋数据集">
                        <p>
                            把螺旋数据集给封装起来<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="49.2">49.2 大型数据集的情况<p class="section"></p>
                <div>
                    <figure>
                        <img src="Resources/img/49-3-大数据集的加载.png" alt="大数据集的加载">
                        <p>
                            不在初始化的时候加载,而是在访问数据的时候加载,这里就是[]的时候加载.<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="49.3">49.3 数据的连接<p class="section"></p>
                <div>
                    <figure>
                        <img src="Resources/img/49-4-数据的连接.png" alt="数据的连接">
                        <p>
                            把数据转换成我们想要的格式<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="49.4">49.4 用于训练的代码<p class="section"></p>
                <div>
                    <figure>
                        <img src="Resources/img/49-5-修改部分.png" alt="修改部分">
                        <p>
                            和上一步相比,我们修改了数据集的导入方式,然后采用小批量取出的方式进行数据加载<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="49.5">49.5 数据集的预处理<p class="section"></p>
                <div>
                    <figure>
                        <img src="Resources/img/49-6-数据预处理.png" alt="数据预处理">
                        <p>
                            添加了对于数据的预处理<br>
                        </p>
                    </figure>
                </div>
            </section>
        </article>
        <article id="50"><h2 class="chapter">50 用于取出小批量数据的DataLoader</h2>
            <section id="50.1">50.1 什么是迭代器<p class="section"></p>
                <div>
                    <figure>
                        <img src="Resources/img/50-1-DataLoader.png" alt="DataLoader">
                        <p>
                            能够重复提取容器中元素的对象<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="50.2">50.2 使用DataLoader<p class="section"></p>
                <div>
                    <figure>
                        <img src="Resources/img/50-2-使用方式.png" alt="使用方式">
                        <p>很方便就能使用DataLaoader加载数据资源<br></p>
                    </figure>
                </div>
            </section>
            <section id="50.3">50.3 accuracy函数的实现<p class="section"></p>
                <div>
                    <figure>
                        <img src="Resources/img/50-3-accuracy.png" alt="accuracy">
                        <p>准备了accuracy函数用来评估计算准确度<br></p>
                    </figure>
                </div>
            </section>
            <section id="50.4">50.4 螺旋数据集的训练代码<p class="section"></p>
                <div>
                    <figure>
                        <img src="Resources/img/50-4-针对测试集和非测试区分处理.png" alt="针对测试集和非测试区分处理">
                        <p>
                            针对不同数据集分开处理<br>
                            <figure>
                                <img src="Resources/img/50-5-处理结果.png" alt="处理结果">
                                <p>
                                    并没有出现过拟合现象(反过来不准确的现象)<br>
                                </p>
                            </figure>
                        </p>
                    </figure>
                </div>
            </section>
        </article>
        <p class="timeStamp" id="20240514">20240514开始</p>
        <article id="51"><p class="chapter">51 MINST的训练</p>
            <section id="51.1">51.1 MNIST数据集<p class="section"></p>
                <figure>
                    <img src="Resources/img/51-1-MNIST数据集.png" alt="MNIST数据集">
                    <p>
                        图像数据集,但是网络异常下不下来<br>
                    </p>
                </figure>
            </section>
            <section id="51.2">51.2 训练MNIST<p class="section"></p>
                <figure>
                    <img src="Resources/img/51-2-训练.png" alt="训练">
                    <p>
                        训练还是老样子,设置多层神经网络,优化参数.模型设定...<br>
                    </p>
                </figure>
            </section>
            <section id="51.3">51.3 改进模型<p class="section"></p>
                <figure>
                    <img src="Resources/img/51-3-新的激活函数.png" alt="新的激活函数">
                    <p>
                        新的激活函数ReLU<br>
                    </p>
                </figure>
            </section>

        </article>
        <p class="timeStamp" id="20240515">20240515开始</p>
        <article id="52"><p class="chapter">52 支持GPU</p>
            <section id="52.1"><p calss="section">52.1 CuPy的安装和使用方法</p>
                <div>
                    <p>
                        原因:深度学习进行的计算大多为矩阵的乘积,矩阵的乘积由加法和乘法组成,这些计算可以并行计算,GPU比CPU更加高效,更擅长.<br>
                        环境支持:从硬件上来说需要Nvida的RTX系列GPU,从软件上来说,Python需要Cupy库<br>
                        <br>
                        Cupy和Numpy的很多API相同,所以我们只要导入cupy然后使用cupy来替换numpy就行了<br>
                        <br>
                        我们需要创建二者的转换机制.<br>
                        Numpy转Cupy需要使用asarray函数<br>
                        Cupy转Numpy需要使用asnumpy函数<br>
                        当调用这两个函数的时候数据会从CPU->GPU或相反,往往这个数据传递过程是深度学习计算的瓶颈,所以理想上要尽可能的减少数据传输.<br>
                        <br>
                        另外一个封装是使用get_array_module函数,根据不同的数据,返回相对应的模块.利用它可以编写同时支持CuPy和NumPy的代码<br>
                    </p>
                </div>
            </section>
            <section id="52.2"><p calss="section">52.2 cuda模块</p>
                <div>
                    <figure>
                        <img src="Resources/img/52-1-导入Cupy.png" alt="导入Cupy">
                        <p>
                            尝试导入Cupy库,如果不行就设置不支持GPU<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/52-2-同时支持.png" alt="同时支持">
                        <p>
                            添加能够根据是Numpy和Cupy返回对应模块的函数<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/52-3-互相转换.png" alt="互相转换">
                        <p>
                            Numpy和Cupy的互相转换函数<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="52.3"><p calss="section">52.3 向Variable/Layer/DataLoader类添加代码</p>
                <div>
                    <figure>
                        <img src="Resources/img/52-4-尝试导入Cupy.png" alt="尝试导入Cupy">
                        <p>
                            尝试导入Cupy,这样Variable检查数据的类型是Cupy还是Numpy<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/52-5-自动识别创建.png" alt="自动识别创建">
                        <p>
                            反向传播也会根据类型创建不同的导数结果<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/52-6-数据传递.png" alt="数据传递">
                        <p>
                            Variable提供cpu和gpu的数据传递函数<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/52-7-数据传递.png" alt="数据传递">
                        <p>
                            Layer当中调用Varaibale提供的数据传递功能<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/52-8-数据传递.png" alt="数据传递">
                        <p>
                            DataLoader当中也要根据数据的不同执行传递<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="52.4"><p calss="section">52.4 函数的相应修改</p>
                <div>
                    <figure>
                        <img src="Resources/img/52-9-函数修改.png" alt="函数修改">
                        <p>
                            所有的函数原本np.xxx的都要修改成这种自动判断类型的<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/52-10-函数修改.png" alt="函数修改">
                        <p>
                            所有的计算函数也需要都添加模块判断<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="52.5"><p calss="section">52.5 在GPU上训练MNIST</p>
                <div>
                    <figure>
                        <img src="Resources/img/52-11-GPU模式.png" alt="GPU模式">
                        <p>
                            最终支持了GPU训练<br>
                        </p>
                    </figure>
                </div>
            </section>
        </article>        
        <article id="53"><p class="chapter">53 模型的保存和加载</p>
            <p class="timeStamp" id="20240516">20240516开始</p>
            <section id="53.1"><p calss="section">53.1 NumPy的save函数和load函数</p>
                <div>
                    将训练过程中的模型保存为"快照",也可以加载训练好的参数,只进行推理.<br>
                    实现:将记录参数的ndarray实例保存到外部文件.<br>
                    <br>
                    因为始终要存到外部,所以不用考虑GPU,GPU数据也要存到CPU然后保存,所以只需要考虑Numpy,然后使用自带的save和load可以保存和读取数据,扩展名是npy,如果使用saves存储多个则扩展名是.npz<br>
                </div>

            </section>
            <section id="53.2"><p calss="section">53.2 Layer类参数的扁平化</p>
                <div>
                    <figure>
                        <img src="Resources/img/53-1-Layer类的扁平化.png" alt="Layer类的扁平化">
                        <p>
                            在layer当中通过字典递归把参数都取出来存到字典里面<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="53.3"><p calss="section">53.3 Layer类的save函数和load函数</p>
                <div>
                    <figure>
                        <img src="Resources/img/53-2-权重存储.png" alt="权重存储">
                        <p>
                            加载和保存函数save_weights和load_wights函数<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/53-3-使用.png" alt="使用">
                        <p>
                            使用很简单,就是调用Layer提供的save_weights和load_wights函数<br>
                        </p>
                    </figure>
                </div>
            </section>
        </article>
        
        <article id="54"><h2 class="chapter">54 Dropout和测试模式</h2>
            <p class="timeStamp" id="20240520">20240520开始</p>
            <section id="54.1"><p class="section">54.1 什么是Dropout</p>
                <p class="paragraph">过拟合的原因</p>
                <p>
                    概念:过拟合是神经网络中常见的问题,主要的原因有以下几点:<br>
                    1.训练数据少<br>
                    2.模型的表现力太强<br>
                    针对第一个原因:我们采取的措施有<strong>增加数据</strong>或使用<strong>数据增强</strong><br>
                    针对第二个原因,可以采取的有效措施有<strong>权重衰减</strong>,<strong>Dropout</strong>和<strong>批量正则化</strong><br>
                </p>
                <p class="paragraph">什么是Dropoutt</p>
                <p>
                    <figure>
                        <img src="Resources/img/54-1-Dropout.png" alt="Dropout">
                        <p>
                            概念:是一种通过随机删除(禁用)神经元进行训练的方法.<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/54-2-Dropout代码.png" alt="Dropout代码">
                    </figure>
                </p>
                <p class="paragraph">集成学习</p>
                <p>
                    集成学习是单独训练多个模型,在推理时对多个模型的输出去平均值的方法.这种做法可以使神经网络的识别精度提高几个百分点.Dropout可以认为在一个网络上达到近似集成学习效果的伪实现.<br>
                </p>
            </section>
            <section id="54.2"><p class="section">54.2 Inverted Dropout</p>
                <p class="paragraph">代码实现</p>
                <p>
                    <figure>
                        <img src="Resources/img/54-3-InvertedDropout.png" alt="InvertedDropout">
                        <p>
                            前面的Dropout比较麻烦的是测试时需要缩放,现在直接在训练的时候把这个处理做了,这样测试的死后就不需要做任何处理了,而且也允许动态改变dropout_ratio的值了<br>
                        </p>
                    </figure>
                </p>
            </section>
            <section id="54.3"><p class="section">54.3 增加测试模式</p>
                <div>
                    <figure>
                        <img src="Resources/img/54-4-区分训练和测试.png" alt="区分训练和测试">
                        <p>
                            增加了训练和测试区分的代码<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="54.4"><p class="section">54.4 Dropout的实现</p>
                <div>
                    <figure>
                        <img src="Resources/img/54-5-添加到function.png" alt="添加到function">
                    </figure>
                    <figure>
                        <img src="Resources/img/54-6-代码使用.png" alt="代码使用">
                        <p>
                            代码使用,训练的时候使用Dropout默认丢掉一半元素训练,测试的时候就不需要了<br>
                        </p>
                    </figure>
                </div>
            </section>
        </article>
        <article id="55"><h2 class="chapter">55 CNN的机制(1)</h2>
            <section id="55.1"><p class="sectino">55.1 CNN的网络结构</p>
                <div>
                    <p class="paragraph">什么是CNN?</p>
                    <p>
                        CNN是Convolutional Neural Network卷积神经网络.<br>
                        它是一种在图像识别,语音识别和自然语言处理等领域使用的神经网络.<br>
                    </p>
                    <p class="paragraph">结构</p>
                    <p>
                        <figure>
                            <img src="Resources/img/55-1-CNN层.png" alt="CNN层">
                            <p>
                                CNN和前面的神经网络相比,多了新的层,<strong>卷积层</strong>(convolution layer)和<strong>池化层</strong>(pooling layer)<br>
                            </p>
                        </figure>
                    </p>                
                </div>
            </section>
            <section id="55.2"><p class="sectino">55.2 卷积运算</p>
                <div>
                    <p class="paragraph">卷积运算</p>
                    <p>
                        <figure>
                            <img src="Resources/img/55-2-卷积运算.png" alt="卷积运算">
                            <p>
                                卷积运算相当于图像中的过滤操作.输入数据4*4大小,用3*3的过滤器(卷积核)和相应数据位置相乘求和得到输出数据.<br>
                            </p>
                        </figure>
                    </p>
                    <p class="paragraph">偏置</p>
                    <p>
                        <figure>
                            <img src="Resources/img/55-3-偏置.png" alt="偏置">
                            <p>
                                这个过滤器在水平和垂直两个维度移动,所以叫做二维卷积层,如果三个维度就叫做三维卷积层,名字分别是Conv2D和Conv3D.<br>
                                在全连接层的神经网络中,除权重层之外还有偏置,卷积层也有<strong>偏置</strong>,就是把数据全部加上偏置值.<br>
                            </p>
                        </figure>
                    </p>
                </div>
            </section>
            <section id="55.3"><p class="sectino">55.3 填充</p>
                <div>
                    <p class="paragraph">填充</p>
                    <p>
                        <figure>
                            <img src="Resources/img/55-4-填充.png" alt="填充">
                            <p>
                                <strong>填充</strong>的主要目的是调整输出的大小,针对重复执行的卷积神经网络来说,需要输入和输出一样大,这样才能重复执行.这就需要填充先变大了<br>
                            </p>
                        </figure>
                    </p>
                </div>
            </section>
            <section id="55.4"><p class="sectino">55.4 步幅</p>
                <div>
                    <p class="paragraph">步幅</p>
                    <p>
                        <figure>
                            <img src="Resources/img/55-5-步幅.png" alt="步幅">
                            <p>
                                应用过滤器之间的间隔被称之为<strong>步幅</strong>,也可以控制输出数据的大小<br>
                            </p>
                        </figure>
                    </p>
                </div>
            </section>
            <section id="55.5"><p class="sectino">55.5 输出大小的计算方法</p>
                <div>
                    <figure>
                        <img src="Resources/img/55-6-计算输出数据大小.png" alt="计算输出数据大小">
                        <p>
                            把计算<strong>输出大小</strong>的程序写成代码<br>
                        </p>
                    </figure>
                </div>
            </section>
        </article>
        <article id="56"><p class="chapter">56 CNN的机制(2)</p>
        <p class="timeStamp" id="20240521">20240521开始</p>
            <section id="56.1"><p class="sectino">56.1 三阶张量</p>
                <figure>
                    <img src="Resources/img/56-1-对三阶张量进行卷积.png" alt="对三阶张量进行卷积">
                    <p>
                        三阶张量也就是三维数据,除了水平和垂直方向,还增加了一个通道方向.<br>
                        输入数据分别和对应的过滤器执行卷积,然后把通道方向上的三个结果<strong>累加压缩</strong>到一起变成输出数据.<br>
                        这里过滤器依然是在水平和垂直两个方向移动,所以虽然是针对三阶张量进行的卷积运算,也叫作二维卷积层.<br>
                    </p>
                </figure>
            </section>
            <section id="56.2"><p class="sectino">56.2 结合方块进行思考</p>
                <div>
                    <figure>
                        <img src="Resources/img/56-2-方块来思考.png" alt="方块来思考">
                        <p>
                            通常情况下都是(channel,height,width)<br>
                            输入数据和过滤器(权重)的通道数是相同的,这个输出数据叫做<strong>特征图</strong><br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/56-3-多个特征图.png" alt="多个特征图">
                        <p>
                            如果我们想要输出多个特征图,就需要多个<strong>过滤器(权重)</strong>,这里我们有OC个C*KH*KW大小的核(过滤器),
                            然后我们再压缩到一个<strong>四阶张量</strong>当中,(output_channel,input_channel,height,width)分别对应的就是(数量,通道数,高度,宽度)<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/56-4-添加偏置.png" alt="添加偏置">
                        <p>
                            添加上偏置项,这里偏置项因为要对OC个特征图分别偏置,所以大小就是OC*1*1了.<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="56.3"><p class="sectino">56.3 小批量处理</p>
                <figure>
                    <img src="Resources/img/56-5-批量处理.png" alt="批量处理">
                    <p>
                        在训练过程中,输入数据会被合并起来进行处理(这种方式就叫做小批量处理),相当于给每个数据又增加了一个新的维度批量大小,变成四阶张量(batch_size,channel,height,width)<br>
                    </p>
                </figure>
            </section>
            <section id="56.4"><p class="sectino">56.4 池化层</p>
                <div>
                    <figure>
                        <img src="Resources/img/56-6-最大值池化.png" alt="最大值池化">
                        <p>
                            池化处理是缩小垂直方向和水平方向空间的操作(减少使用数据的量).<br>
                            典型的池化有最大值池化(上图)和平均值池化.简单来说就是取池化区域(上图是2*2)中的最大值/平均值.<br>
                            通常来说池化层的大小和步幅相同.<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/56-7-池化特性.png" alt="池化特性">
                        <p>
                            池化层具有以下特点<br>
                            1.<strong>没有任何学习参数</strong>,只是单纯的取最大值(最大池化)或平均值(平均池化)<br>
                            2.<strong>通道数量不发生变化</strong>,池化不需要输出数据的时候在通道方向相加,所以不会改变通道数<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/56-8-稳定性.png" alt="稳定性">
                        <p>
                            3.<strong>对微小差异具有稳定性</strong>,在最大池化中,只要最大值不变,其余值改变不影响,在平均池化中,只要改变不是特别大,影响也微乎其微.<br>
                        </p>
                    </figure>
                </div>
            </section>
        </article>
        <article id="57"><p class="chapter">57 conv2d函数和pooling函数</p>
            <section id="57.1"><p class="sectino">57.1 使用im2col展开</p>
                <div>
                    <figure>
                        <img src="Resources/img/57-1-展开卷积核.png" alt="展开卷积核">
                        <p>
                            我们使用Numpy当中的im2col来实现将输入数据转换成卷积核易于处理的形式(矩阵).<br>
                            Chainer的Im2col并没有执行到reshape这一步,因为他也可以处理张量积(就是一列的矩阵)<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/57-2-计算逻辑.png" alt="计算逻辑">
                        <p>
                            计算逻辑是通过im2col函数转换成矩阵,然后匹配矩阵乘法把卷积核也扩展成一列,
                            这样两个矩阵相乘得到了输出数据矩阵,最后把矩阵重新映射回包括批量的四阶张量.<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="57.2"><p class="sectino">57.2 conv2d函数的实现</p>
                <div>
                    <figure>
                        <img src="Resources/img/57-3-im2col.png" alt="im2col">
                        <p>
                            im2col函数我们不关心内部实现,知道有哪些参数可以设置就好了<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/57-4-conv2d.png" alt="conv2d">
                    </figure>
                </div>
            </section>
            <section id="57.3"><p class="sectino">57.3 Conv2d层的实现</p>
                <figure>
                    <img src="Resources/img/57-5-Conv2d.png" alt="Conv2d">
                    <p>
                        层的实现,本质上来说只是为了方便管理参数<br>
                    </p>
                </figure>
            </section>
            <section id="57.4"><p class="sectino">57.4 pooling函数的实现</p>
                <div>
                    <figure>
                        <img src="Resources/img/57-6-池化数据展开.png" alt="池化数据展开">
                        <p>
                            池化层的特性是不改变通道数,所以我们按照通道方向C进行展开,把每个池化核心KH*KW按照通道方向C展开成一排,
                            然后我们再按照池化核心的数量把每个核一排一排往下排形成列.<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/57-7-池化实现.png" alt="池化实现">
                        <p>
                            经过前面的步骤展开之后,我们就对每一排(每个核心)进行池化操作(这里是取最大)就得到了池化的结果列,
                            最后再重新映射回数据数据形状就好了.<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/57-8-池化代码.png" alt="池化代码">
                    </figure>
                </div>
            </section>
        </article>

        <article id="58"><p class="chapter">58 具有代表性的CNN(VGG16)</p>
            <p class="timeStamp" id="20240522">20240522开始</p>
            <section id="58.1"><p class="sectino">58.1 VGG16的实现</p>
                <div>
                    <figure>
                        <img src="Resources/img/58-1-VGG16.png" alt="VGG16">
                        <p>
                            3*3 Conv 64 表示3*3的卷积核,输出通道数为64(那就意味着有64个核)<br>
                            pool/2 表示2*2的池化层<br>
                            linear 4096表示输出大小为4096的全连接层<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/58-2-代码1.png" alt="代码1">
                        <p>
                            特点:<br>
                            使用3*3的卷积层(填充为1*1)<br>
                            卷积层的通道数量基本上每次池化之后都翻倍<br>
                            在全连接层使用Dropout来减少过拟合<br>
                            使用ReLU作为激活函数.<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="58.2"><p class="sectino">58.2 已训练的权重数据</p>
                <div>
                    <figure>
                        <img src="Resources/img/58-3-使用权重.png" alt="使用权重">
                        <p>
                            下载了别人训练好的识别权重,作者转换成了Dezero可以使用的加载<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="58.3"><p class="sectino">58.3 使用已训练的VGG16</p>
                <div>
                    <figure>
                        <img src="Resources/img/58-4-使用VGG16.png" alt="使用VGG16">
                        <p>
                            使用的PIL库的Image类来读取图像<br>
                            preprocess吹图像,调整到224高224宽转换为ndarray实例<br>
                            np.newaxis是增加一个批量的轴<br>
                            最后的plot是绘制一个计算图.<br>
                            最终识别出来结果是zebra斑马<br>
                        </p>
                    </figure>
                </div>
            </section>
        </article>
        <article id="59"><p class="chapter">59 使用RNN处理时间序列数据</p>
            <section id="59.1"><p class="sectino">59.1 RNN层的实现</p>
                <div>
                    <figure>
                        <img src="Resources/img/59-1-RNN.png" alt="RNN">
                        <p>
                            RNN具有循环神经网络结构,会利用上一次计算的结果来辅助下一次的计算(这和图形抗锯齿当中的TAA想法差不多)<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/59-2-代码逻辑.png" alt="代码逻辑">
                        <p>
                            有两个权重,一个是上一次的结果用,一个是这次计算结果用,最后加上偏置b.最后用tanh映射到(-1,1)范围内<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/59-3-实现逻辑.png" alt="实现逻辑">
                        <p>
                            第二次计算通过h2h用到了第一次计算的结果h作为参数<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="59.2"><p class="sectino">59.2 RNN模型的实现</p>
                <div>
                    <figure>
                        <img src="Resources/img/59-4-RNN的实现.png" alt="RNN的实现">
                        <p>
                            这种由一系列的输入数据组成的计算图上进行反向传播叫做基于时间的反向传播BPTT<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="59.3"><p class="sectino">59.3 切断连接的方式</p>
                <div>
                    <figure>
                        <img src="Resources/img/59-5-需要切断.png" alt="需要切断">
                        <p>
                            这个传递过程中需要切断,当使用了上一次计算的结果,上上次及之前的隐藏状态就需要切断舍弃了<br>
                        </p>
                    </figure>
                    <figure>
                        <img src="Resources/img/59-6-切断实现.png" alt="切断实现">
                        <p>
                            切断的实现通过断开对创建函数的指针<br>
                            在unchain_backward代码中遍历所有的函数执行切断<br>
                        </p>
                    </figure>
                </div>
            </section>
            <section id="59.4"><p class="sectino">59.4 正弦波的预测</p>
                <div>
                    <figure>
                        <img src="Resources/img/59-7-使用.png" alt="使用">
                        <p>
                            也是一种算法,能用于和上一次的数据建立联系.<br>
                        </p>
                    </figure>
                </div>
            </section>
        </article>
        <article id="60"><p class="chapter">60 LSTM与数据加载器</p>
            <section id="60.1"><p class="sectino">60.1 用于时间序列数据的数据加载器</p></section>
            <section id="60.2"><p class="sectino">60.2 LSTM层的实现</p></section>
        </article>
        <footer><a href="../../../../index.html">首页</a></footer>
        <script>
                // 读取存储的滚动位置并滚动到该位置  
            const scrollPositionKey = 'scrollPosition';  // 定义滚动位置变量
            const storedScrollPosition = localStorage.getItem(scrollPositionKey); // localStorage是存在用户设备上
            
            window.onload = function()  // 读取窗口的时候就回滚到上次位置
            {
                if (storedScrollPosition)  // 如果滚动位置存在 
                {  
                    window.scrollTo(0, parseInt(storedScrollPosition)); // 滚动到这个位置 
                } 
            } 
            
            // 监听滚动事件并存储位置  
            window.addEventListener('scroll', () => 
            {  
                const scrollPosition = window.scrollY || window.pageYOffset || document.body.scrollTop + (document.documentElement && document.documentElement.scrollTop || 0);  
                localStorage.setItem(scrollPositionKey, scrollPosition);  
            });
        </script>
    </body>
</html>