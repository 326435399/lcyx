<!DOCTYPE html>  <!--也可以写成小写doctype,这个标签告诉浏览器这是一个HTML5文档-->
<html lang="en">  <!--结构_HTML主体,属性lang设置语言-->
    <head>  <!--结构_Head抬头-->
        <meta charset="UTF-8">  <!--编码格式属性charset-->
        <meta name="StrawBear" content="记录一个学习者的过程">
        <title>20240403-深度学习入门_自制框架</title>  <!--标题-->
        <link rel="stylesheet" href="../../../1_CSS/basicStyle.css">
    </head>
    <body>  <!--结构_Body-->
        <header><h1>20240403-深度学习入门_自制框架</h1></header>  <!--header(页眉)是介绍性内容的容器-->
        <nav></nav>  <!--nav是navigation的缩写,导航元素,通常是引向其他资源的链接之类的-->
        <aside></aside>  <!--aside侧边栏,通常是一些相关主题或其他-->
        <article>
            <section>29.使用牛顿法进行优化(手动计算)</section>
            <p> <!--每个p里面是小主题-->
                <div>
                    问题引入:在上一章我们需要5万次迭代Rosenbrock函数才最终收敛,这个速度太慢了,所以我们就寻找收敛比较快的方法来替代他,这个方法就是牛顿法,一般来说如果初始值与解足够接近,牛顿法能够较快的收敛.<br>
                    <br>
                    29.1 使用牛顿法进行优化的理论知识<br>
                    简化聚焦:为了聚焦于核心,我们使用最简单的单一变量函数y=f(x),通过泰勒展开我们将f表示为以某一点a(图里面的x0)为起点的x的多项式.在多项式中,一阶导,二阶导,三阶导...不断的增加对于这个点的描述信息,一阶导给出了切线斜率信息,二阶导给出了上凹还是下凹,后面就不太清楚了.<br>
                    <img src="../../../0_Resoureces/img/learningPicture/20240403/5_DeepLearning_Python/20240403_5_01 泰勒多项式.png" alt="picture"><br>
                    近似:因为对于曲线的影响随着阶数的增加越来越小,所以我们取前面两阶就已经能够有足够好的近似了,(其实还有一个误差计算公式来计算需要几阶的泰勒多项式能达到目标精度),这种取泰勒多项式前两阶的近似叫做二次近似.<br>
                    <img src="../../../0_Resoureces/img/learningPicture/20240403/5_DeepLearning_Python/20240403_5_02 二次近似.png" alt="picture"><br>              
                    求最小值:比较幸运的是二次函数我们知道是一个回形针一样的上凹曲线,它的最小值是有解析解的,就在最下面,导数为0的地方,所以我们计算求出来这个最小值的解析解.<br>
                    <img src="../../../0_Resoureces/img/learningPicture/20240403/5_DeepLearning_Python/20240403_5_03 求二次函数解析解.png" alt="picture"><br>
                    计算过程<br>
                    <img src="../../../0_Resoureces/img/learningPicture/20240403/5_DeepLearning_Python/20240403_5_04 计算过程.png" alt="picture"><br>
                    更新a的值,然后再次计算,这就是使用牛顿法进行优化的方法.<br>
                    <img src="../../../0_Resoureces/img/learningPicture/20240403/5_DeepLearning_Python/20240403_5_05 更新位置.png" alt="picture"><br>
                    上面的公式是使用梯度下降的方法,往梯度方向移动,系数α是我们指定的学习率,下面的公式是牛顿迭代法,通过二阶导数自动调整移动的距离.我们可以把牛顿迭代法看成是α=1/f''(x)的梯度下降法.<br>
                    <img src="../../../0_Resoureces/img/learningPicture/20240403/5_DeepLearning_Python/20240403_5_06 对比梯度下降和牛顿迭代法.png" alt="picture"><br>
                    效率:梯度下降只利用到了一阶导数信息来优化,而牛顿法利用了一阶和二阶导数信息来优化.<br>
                    <br>
                    29.2 使用牛顿法实现优化<br>
                    目的:Dezero现在我们还没有写计算二阶导数的方法,所以我们先手动计算来测试一下.<br>
                    <img src="../../../0_Resoureces/img/learningPicture/20240403/5_DeepLearning_Python/20240403_5_07 手动计算示例的二阶导.png" alt="picture"><br>
                    测试代码:我们发现只要7次,牛顿法就收敛了<br>
                    <img src="../../../0_Resoureces/img/learningPicture/20240403/5_DeepLearning_Python/20240403_5_08 测试代码.png" alt="picture"><br>
                    牛顿迭代法和梯度下降法的对比:7比124<br>
                    <img src="../../../0_Resoureces/img/learningPicture/20240403/5_DeepLearning_Python/20240403_5_09 对比结果.png" alt="picture"><br>
                </div>
            </p>
        </article>
        <footer><a href="https://326435399.github.io/lcyx/index.html">首页</a></footer>  <!--footer(页脚)是额外信息,例如:链接,版权说明,作者等等-->
        <script>
            document.write(document.lastModified);  // 打印上一次的修改时间
        </script>
    </body>
</html>